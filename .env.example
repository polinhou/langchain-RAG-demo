# Ollama Configuration
# Copy this file to .env and update the values as needed

# For Docker Compose, use host.docker.internal to connect to host's Ollama
# For local development, use localhost
OLLAMA_EMBEDDINGS_URL=http://host.docker.internal:11434
OLLAMA_CHAT_URL=http://host.docker.internal:11434

# Model to use for embeddings (e.g., bge-m3:latest, nomic-embed-text:latest, etc.)
OLLAMA_EMBEDDINGS_MODEL=bge-m3:latest

# Model to use for chat (e.g., gemma3:latest, llama3:latest, etc.)
OLLAMA_CHAT_MODEL=gemma3:latest

# Qdrant Configuration
# When using Docker Compose, use the service name 'qdrant' as the hostname
# For local development, use localhost
QDRANT_URL=http://qdrant:6333
COLLECTION_NAME=qa_collection

# Application Configuration
# Directory containing PDF files to process
# This path is relative to the host when using Docker
DIRPATH=./data

# Note: Never commit your actual .env file to version control!
# Make sure to create a .env file with your actual values

# Docker-specific notes:
# - host.docker.internal resolves to the host machine's IP address from inside containers
# - The data directory will be mounted from the host to the container
# - Qdrant data will be persisted in a Docker volume named 'ollama-embedding-demo_qdrant_storage'
